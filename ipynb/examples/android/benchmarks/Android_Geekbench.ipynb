{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Geekbench benchmark on Android\n",
    "\n",
    "Geekbench4 is an app offering several benchmarks to run on android smartphones. The one used in this notebook is the '**CPU**' benchmark, which runs several workloads that follow the lines of what is commonly run by smartphones (AES, JPEG codec, FFT, and so on). The benchmark runs all the tests in '**Single-Core**' mode as well as in '**Multi-Core**' in order to compare the single-thread and multi-thread performances of the device.\n",
    "\n",
    "**Do note that the benchmark will attempt to upload its results, which includes some hardware information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from conf import LisaLogging\n",
    "LisaLogging.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Support to access the remote target\n",
    "import devlib\n",
    "from env import TestEnv\n",
    "\n",
    "# Import support for Android devices\n",
    "from android import Screen, Workload\n",
    "\n",
    "# Support for trace events analysis\n",
    "from trace import Trace\n",
    "\n",
    "# Suport for FTrace events parsing and visualization\n",
    "import trappy\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Support Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This function helps us run our experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def experiment():\n",
    "    \n",
    "    # Configure governor\n",
    "    target.cpufreq.set_all_governors('schedutil')\n",
    "    \n",
    "    # Get workload\n",
    "    wload = Workload.getInstance(te, 'Geekbench')\n",
    "    \n",
    "    # Run Geekbench workload\n",
    "    wload.run(te.res_dir, test_name='CPU', collect='ftrace')\n",
    "        \n",
    "    # Dump platform descriptor\n",
    "    te.platform_dump(te.res_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Test environment setup\n",
    "For more details on this please check out **examples/utils/testenv_example.ipynb**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**devlib** requires the ANDROID_HOME environment variable configured to point to your local installation of the Android SDK. If you have not this variable configured in the shell used to start the notebook server, you need to run a cell to define where your Android SDK is installed or specify the ANDROID_HOME in your target configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In case more than one Android device are conencted to the host, you must specify the ID of the device you want to target in **my_target_conf**. Run **adb devices** on your host to get the ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Setup target configuration\n",
    "my_conf = {\n",
    "\n",
    "    # Target platform and board\n",
    "    \"platform\"     : 'android',\n",
    "    \"board\"        : 'pixel',\n",
    "    \n",
    "    # Device\n",
    "    \"device\"       : \"HT67M0300128\",\n",
    "    \n",
    "    # Android home\n",
    "    \"ANDROID_HOME\" : \"/home/vagrant/lisa/tools/android-sdk-linux/\",\n",
    "\n",
    "    # Folder where all the results will be collected\n",
    "    \"results_dir\" : \"Geekbench_example\",\n",
    "\n",
    "    # Define devlib modules to load\n",
    "    \"modules\"     : [\n",
    "        'cpufreq'       # enable CPUFreq support\n",
    "    ],\n",
    "\n",
    "    # FTrace events to collect for all the tests configuration which have\n",
    "    # the \"ftrace\" flag enabled\n",
    "    \"ftrace\"  : {\n",
    "         \"events\" : [\n",
    "            \"sched_switch\",\n",
    "            \"sched_wakeup\",\n",
    "            \"sched_wakeup_new\",\n",
    "            \"sched_overutilized\",\n",
    "            \"sched_load_avg_cpu\",\n",
    "            \"sched_load_avg_task\",\n",
    "            \"cpu_capacity\",\n",
    "            \"cpu_frequency\",\n",
    "         ],\n",
    "         \"buffsize\" : 100 * 1024,\n",
    "    },\n",
    "\n",
    "    # Tools required by the experiments\n",
    "    \"tools\"   : [ 'trace-cmd', 'taskset'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize a test environment using:\n",
    "te = TestEnv(my_conf, wipe=False)\n",
    "target = te.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Workloads execution\n",
    "\n",
    "This is done using the **experiment** helper function defined above which is configured to run a **Geekbench - CPU** experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initialize Workloads for this test environment\n",
    "results = experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Results collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Geekbench4 uses a baseline score of 4000, which is the benchmark score of an Intel Core i7-6600U. Higher scores are better, with double the score indicating double the performance. You can have a look at the results for several android phones here https://browser.primatelabs.com/android-benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper class will be used to parse the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Geekbench(object):\n",
    "    \n",
    "    def __init__(self, filepath):\n",
    "        with open(filepath) as fd:\n",
    "            self.__json = json.loads(fd.read())\n",
    "        \n",
    "        self.__benchmarks = {}\n",
    "        for section in self.__json[\"sections\"]:\n",
    "            self.__benchmarks[section[\"name\"]] = section\n",
    "            for workload in section[\"workloads\"]:\n",
    "                self.__benchmarks[section[\"name\"]][workload[\"name\"]] = workload\n",
    "            \n",
    "            \n",
    "    def name(self):\n",
    "        gov = \"\"\n",
    "        build = \"\"\n",
    "        for metric in self.__json[\"metrics\"]:\n",
    "            if metric[\"name\"] == \"Governor\":\n",
    "                gov = metric[\"value\"]\n",
    "            elif metric[\"name\"] == \"Build\":\n",
    "                build = metric[\"value\"]\n",
    "\n",
    "        return \"[build]=\\\"{}\\\" [governor]=\\\"{}\\\"\".format(build, gov)\n",
    "    \n",
    "    def __benchmarks_names(self):\n",
    "        return [section[\"name\"] for section in self.__json[\"sections\"]]\n",
    "    \n",
    "    def __workloads_names(self):\n",
    "        return [workload[\"name\"] for workload in self.__benchmarks.values()[0][\"workloads\"]]\n",
    "    \n",
    "    def global_results(self):\n",
    "        data = []\n",
    "        for benchmark in self.__benchmarks_names():\n",
    "            data.append(self.__benchmarks[benchmark][\"score\"])\n",
    "        df = pd.DataFrame(data, index=self.__benchmarks_names(), columns=[\"Score\"])\n",
    "        return df\n",
    "        \n",
    "    def detailed_results(self):\n",
    "        benchmark_fields = [\"score\", \"runtime_mean\", \"rate_string\"]\n",
    "        dfs = {}\n",
    "\n",
    "        benchmarks = self.__benchmarks_names()\n",
    "        workloads = self.__workloads_names() \n",
    "        for benchmark in benchmarks:\n",
    "            data = []\n",
    "            idx = []\n",
    "            for workload in workloads:\n",
    "                wl_data = []\n",
    "                for field in benchmark_fields:\n",
    "                    wl_data.append(self.__benchmarks[benchmark][workload][field])            \n",
    "                data.append(tuple(wl_data))\n",
    "                idx.append(workload)  \n",
    "\n",
    "            df = pd.DataFrame(data, index=idx, columns=benchmark_fields)\n",
    "            dfs[benchmark] = df\n",
    "            \n",
    "        return dfs\n",
    "        \n",
    "    def compare_global(self, other):\n",
    "        bench_types = self.__benchmarks_names()\n",
    "        columns = [\"New score\", \"Previous score\", \"Delta (%)\"]\n",
    "        data = []\n",
    "\n",
    "        for bench_type in bench_types:\n",
    "            other_score = other.__benchmarks[bench_type][\"score\"]\n",
    "            my_score = self.__benchmarks[bench_type][\"score\"]\n",
    "            delta = (1.0 * my_score - other_score) / other_score\n",
    "            data.append((my_score, other_score, float(\"{:.2f}\".format(delta * 100))))\n",
    "            \n",
    "        df = pd.DataFrame(data, index=bench_types, columns=columns)\n",
    "        df.sort_values(by=columns[-1], inplace=True)\n",
    "        return df\n",
    "        \n",
    "    def compare_detailed(self, other):\n",
    "        bench_types = self.__benchmarks_names()\n",
    "        columns = [\"New score\", \"Previous score\", \"Delta (%)\"]\n",
    "        workloads = self.__workloads_names()\n",
    "        dfs = {}\n",
    "        \n",
    "        for bench_type in bench_types:\n",
    "            data = []\n",
    "            idx = []\n",
    "            other_bench = other.__benchmarks[bench_type]\n",
    "            my_bench = self.__benchmarks[bench_type]\n",
    "            \n",
    "            for workload in workloads:\n",
    "                other_score = other_bench[workload][\"score\"]\n",
    "                my_score = my_bench[workload][\"score\"]\n",
    "                delta = (1.0 * my_score - other_score) / other_score\n",
    "                data.append((my_score, other_score, float(\"{:.2f}\".format(delta * 100))))\n",
    "                idx.append(workload)\n",
    "\n",
    "            df = pd.DataFrame(data, index=idx, columns=columns)\n",
    "            df.sort_values(by=columns[-1], inplace=True)\n",
    "            dfs[bench_type] = df\n",
    "            \n",
    "        return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class TE():\n",
    "    res_dir=\"/home/valsch01/Work/lisa/results/hikey_geekbench_eas\"\n",
    "    \n",
    "te = TE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare_dirs = [ \"/home/valsch01/Work/lisa/ipynb/scratchpad/geekbench4/sched\" ]\n",
    "\n",
    "compare_geekbenches = []\n",
    "for d in compare_dirs:\n",
    "    files = [f for f in os.listdir(d) if f.endswith(\".gb4\")]\n",
    "    for f in files:\n",
    "        compare_geekbenches.append(Geekbench(d + \"/\" + f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we parse the results from the benchmark run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_bench_results(geekbench, detailed=False):\n",
    "    print \"===== Global results =====\"\n",
    "    display(geekbench.global_results())\n",
    "    \n",
    "    if not detailed:\n",
    "        return\n",
    "    \n",
    "    print \"===== Detailed results =====\"\n",
    "    for benchmark, df in geekbench.detailed_results().iteritems():\n",
    "        print \"----- {} benchmark -----\".format(benchmark)\n",
    "        display(df)\n",
    "    \n",
    "def compare_bench_results(geekbench, other, detailed=False):\n",
    "    print \"===== Global results =====\"\n",
    "    display(geekbench.compare_global(other))\n",
    "        \n",
    "    if not detailed:\n",
    "        return\n",
    "        \n",
    "    print \"===== Detailed results =====\"\n",
    "    dfs = geekbench.compare_detailed(other)\n",
    "    for name, df in dfs.iteritems():\n",
    "        print \"----- {} -----\".format(name)\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for f in os.listdir(te.res_dir):\n",
    "    if f.endswith(\".gb4\"):\n",
    "        geekbench = Geekbench(te.res_dir + \"/\" + f)\n",
    "        \n",
    "        print \"Analysing geekbench {}\".format(geekbench.name())\n",
    "        display_bench_results(geekbench)\n",
    "        \n",
    "        for other in compare_geekbenches:\n",
    "            print \"Comparing to {}\".format(other.name())\n",
    "            compare_bench_results(geekbench, other)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
